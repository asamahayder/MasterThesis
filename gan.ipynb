{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to test the data augmentation technique from this paper: https://onlinelibrary.wiley.com/doi/full/10.1002/int.23013\n",
    "\n",
    "The topics for investigation are:\n",
    "- How effective is this technique at generating new samples for our dataset\n",
    "- How long does it take to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ex-vivo data\n",
    "path_to_data = \"Data\\\\Ex-Vivo\\\\\"\n",
    "files = os.listdir(path_to_data)\n",
    "\n",
    "# Removing readme\n",
    "files.remove(\"READ_ME.txt\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(path_to_data + file, 'rb') as f:\n",
    "        data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1 bare\n"
     ]
    }
   ],
   "source": [
    "print(data[1]['samplematrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (92, 1100)\n",
      "Y shape:  (92,)\n",
      "y values first 10:  [2 2 2 2 2 2 2 2 3 1]\n",
      "distinct y values:  [0 1 2 3]\n",
      "X dtype:  float32\n",
      "Y dtype:  int64\n"
     ]
    }
   ],
   "source": [
    "# These values were found by experimenting and inspecting the resulting pulses\n",
    "window_start = 3750\n",
    "window_end = 4850\n",
    "\n",
    "data = [d for d in data if 'sample' in d['samplematrix']]\n",
    "X = [d['scan'][0]['forward_scan']['signal'][window_start:window_end] for d in data]\n",
    "Y = [d['samplematrix'].split()[2] for d in data]\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "print(\"y values first 10: \", Y[:10])\n",
    "print(\"distinct y values: \", np.unique(Y))\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "print(\"X dtype: \", X.dtype)\n",
    "print(\"Y dtype: \", Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asama\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\asama\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asama\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.7829387187957764, acc.: 45.3125%] [G loss: [array(0.78807676, dtype=float32), array(0.78807676, dtype=float32), array(0.40625, dtype=float32)]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'treated_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 128\u001b[0m\n\u001b[0;32m    125\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Save generated pulse samples\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43msave_pulses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 103\u001b[0m, in \u001b[0;36msave_pulses\u001b[1;34m(epoch, n)\u001b[0m\n\u001b[0;32m    101\u001b[0m gen_pulses \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pulse \u001b[38;5;129;01min\u001b[39;00m gen_pulses:\n\u001b[1;32m--> 103\u001b[0m     \u001b[43msave_pulse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpulse\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 106\u001b[0m, in \u001b[0;36msave_pulse\u001b[1;34m(epoch, pulse)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pulse\u001b[39m(epoch, pulse):\n\u001b[1;32m--> 106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtreated_data\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_cut\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    107\u001b[0m     y \u001b[38;5;241m=\u001b[39m pulse\n\u001b[0;32m    109\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'treated_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, BatchNormalization, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your dataset\n",
    "# For demonstration, let's create a dummy dataset\n",
    "# Replace this with loading your actual dataset\n",
    "pulses = np.random.rand(44, 1100)  # 44 pulses, each with 1100 features\n",
    "\n",
    "# Normalize the dataset\n",
    "pulses = (pulses - 0.5) / 0.5\n",
    "\n",
    "# Define GAN components\n",
    "latent_dim = 100\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1100, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(1100,)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates pulses\n",
    "z = tf.keras.Input(shape=(latent_dim,))\n",
    "pulse = generator(z)\n",
    "\n",
    "# For the combined model, only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated pulses as input and determines validity\n",
    "validity = discriminator(pulse)\n",
    "\n",
    "# Combined model (stacked generator and discriminator)\n",
    "combined = tf.keras.Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training the GAN\n",
    "def train(epochs, batch_size=32, save_interval=50):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, pulses.shape[0], batch_size)\n",
    "        real_pulses = pulses[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_pulses = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_pulses, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_pulses, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "\n",
    "        # Save generated pulse samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_pulses(epoch)\n",
    "\n",
    "def save_pulses(epoch, n=5):\n",
    "    noise = np.random.normal(0, 1, (n, latent_dim))\n",
    "    gen_pulses = generator.predict(noise)\n",
    "    for pulse in gen_pulses:\n",
    "        save_pulse(epoch, pulse)\n",
    "\n",
    "def save_pulse(epoch, pulse):\n",
    "    x = treated_data[0]['time_cut']\n",
    "    y = pulse\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.plot(x, y, label=\"signal\")\n",
    "    plt.xlabel(\"Time(s)\")\n",
    "    plt.ylabel(\"Signal (nA)\")\n",
    "    plt.title(f'Generated Signal from epoch: {epoch}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = f'generated/epoch_{epoch}'\n",
    "\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.savefig(f\"{path}/{str(uuid.uuid4())[:8]}\")\n",
    "    plt.close()\n",
    "\n",
    "# Train the GAN\n",
    "train(epochs=1000, batch_size=16, save_interval=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does only output random noise, does no seem to learn any patterns. But at least, the noise is not identical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
